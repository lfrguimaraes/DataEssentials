{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Template\n",
    "\n",
    "Let's build our first Machine Learning model: *Linear Regression*. This is a template you can reuse when you'll be building your own models. \n",
    "\n",
    "## What you will learn in this course üßêüßê\n",
    "\n",
    "* Create linear regressions using `sklearn` \n",
    "* Visualize your model's performance with `matplotlib`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Preprocessing üç≥\n",
    "\n",
    "As you already know, there is always a preprocessing step that includes: \n",
    "\n",
    "* Importing libraries \n",
    "* Importing data \n",
    "* Seperate target & feature variable \n",
    "* Split data into training and testing set\n",
    "* Standardize data \n",
    "\n",
    "\n",
    "Let's do that here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>YearsExperience</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>1.1</td>\n",
       "      <td>39343.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United-Kingdom</td>\n",
       "      <td>1.3</td>\n",
       "      <td>46205.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>France</td>\n",
       "      <td>1.5</td>\n",
       "      <td>37731.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>France</td>\n",
       "      <td>2.0</td>\n",
       "      <td>43525.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>2.2</td>\n",
       "      <td>39891.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Country  YearsExperience   Salary\n",
       "0          France              1.1  39343.0\n",
       "1  United-Kingdom              1.3  46205.0\n",
       "2          France              1.5  37731.0\n",
       "3          France              2.0  43525.0\n",
       "4         Germany              2.2  39891.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import & visualize dataset\n",
    "df = pd.read_csv(\"src/salary_data.csv\")\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 3)\n",
      "               Country  YearsExperience         Salary\n",
      "count               30        30.000000      30.000000\n",
      "unique               3              NaN            NaN\n",
      "top     United-Kingdom              NaN            NaN\n",
      "freq                12              NaN            NaN\n",
      "mean               NaN         5.313333   76003.000000\n",
      "std                NaN         2.837888   27414.429785\n",
      "min                NaN         1.100000   37731.000000\n",
      "25%                NaN         3.200000   56720.750000\n",
      "50%                NaN         4.700000   65237.000000\n",
      "75%                NaN         7.700000  100544.750000\n",
      "max                NaN        10.500000  122391.000000\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of dataset in the form of (#rows, #columns)\n",
    "print(df.shape)\n",
    "\n",
    "# Describe dataset's main statistics\n",
    "print(df.describe(include=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separating target variable from features...\n",
      "...Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Separate target variable Y from features X\n",
    "print(\"Separating target variable from features...\")\n",
    "\n",
    "## Choose the columns you want to have as your features\n",
    "features_list = [\"Country\",\"YearsExperience\"]\n",
    "\n",
    "X = df.loc[:,features_list] # We add feature_list into our loc \n",
    "y = df.loc[:,\"Salary\"] # We set \"Salary\" as the target variable\n",
    "\n",
    "print(\"...Done.\")\n",
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëã You could have used `iloc` as well. Whatever is handy for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset into train set and test set...\n",
      "...Done.\n"
     ]
    }
   ],
   "source": [
    "# Divide dataset Train set & Test set \n",
    "## First we import train_test_split\n",
    "\n",
    "\n",
    "print(\"Splitting dataset into train set and test set...\")\n",
    "## Then we use train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=0)\n",
    "\n",
    "print(\"...Done.\")                                                                "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëã `test_size=0.2` is completely arbitrary. You choose whaveter proportion suits you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training pipeline ---\n",
      "\n",
      "#### X_train BEFORE preprocessing ####\n",
      "           Country  YearsExperience\n",
      "27  United-Kingdom              9.6\n",
      "11  United-Kingdom              4.0\n",
      "17  United-Kingdom              5.3\n",
      "22          France              7.9\n",
      "5   United-Kingdom              2.9\n",
      "\n",
      "Encoding categorical features and standardizing numerical features...\n",
      "...Done.\n",
      "#### X_train AFTER preprocessing ####\n",
      "[[ 0.          0.          1.          1.75832984]\n",
      " [ 0.          0.          1.         -0.40973925]\n",
      " [ 0.          0.          1.          0.09356251]\n",
      " [ 1.          0.          0.          1.10016601]\n",
      " [ 0.          0.          1.         -0.83560996]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Training pipeline ###\n",
    "print(\"--- Training pipeline ---\")\n",
    "print()  \n",
    "\n",
    "# Encoding categorical features and standardizing numeric features\n",
    "\n",
    "print(\"#### X_train BEFORE preprocessing ####\")\n",
    "print(X_train.head())\n",
    "print()\n",
    "\n",
    "print(\"Encoding categorical features and standardizing numerical features...\")\n",
    "## First let's import libraries\n",
    "## StandardScaler to scale data (i.e apply Z-score)\n",
    "## OneHotEncoder to encode categorical variables\n",
    "\n",
    "\n",
    "numeric_features = [1] # Choose which column index we are going to scale\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "categorical_features = [0] \n",
    "categorical_transformer = OneHotEncoder()\n",
    "\n",
    "\n",
    "# Apply ColumnTransformer to create a pipeline that will apply the above preprocessing\n",
    "feature_encoder = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features),    \n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "X_train = feature_encoder.fit_transform(X_train)\n",
    "print(\"...Done.\")\n",
    "print(\"#### X_train AFTER preprocessing ####\")\n",
    "print(X_train[0:5,:]) # print first 5 rows (not using iloc since now X_train became a numpy array)\n",
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Build the model üèãÔ∏è‚Äç‚ôÇÔ∏è\n",
    "\n",
    "Let's now create your model. With `sklearn`, you'll see that it's not that long! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model...\n",
      "...Done.\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "print(\"Train model...\")\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train) # This steps is the actual training !\n",
    "print(\"...Done.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now test our model and see its predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on training set...\n",
      "...Done.\n",
      "[118002.7170578   66176.7536895   78207.7809      99506.0692039\n",
      "  55996.65374215]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predictions on training set\n",
    "print(\"Predictions on training set...\")\n",
    "y_train_pred = regressor.predict(X_train)\n",
    "print(\"...Done.\")\n",
    "print(y_train_pred[:5]) # print first 5 rows (not using iloc since now y_train became a numpy array)\n",
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now reproduce the same process on the test set and compare performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing pipeline ---\n",
      "Standardizing numerical features...\n",
      "           Country  YearsExperience\n",
      "2           France              1.5\n",
      "28          France             10.3\n",
      "13          France              4.1\n",
      "10  United-Kingdom              3.9\n",
      "26  United-Kingdom              9.5\n",
      "24          France              8.7\n",
      "\n",
      "...Done.\n",
      "[[ 1.          0.          0.         -1.37762723]\n",
      " [ 1.          0.          0.          2.02933848]\n",
      " [ 1.          0.          0.         -0.37102373]\n",
      " [ 0.          0.          1.         -0.44845477]\n",
      " [ 0.          0.          1.          1.71961432]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Testing pipeline ###\n",
    "print(\"--- Testing pipeline ---\")\n",
    "\n",
    "# Standardizing numeric features\n",
    "print(\"Standardizing numerical features...\")\n",
    "print(X_test)\n",
    "print()\n",
    "\n",
    "X_test = feature_encoder.transform(X_test)\n",
    "\n",
    "print(\"...Done.\")\n",
    "print(X_test[:5]) # print first 5 rows (not using iloc since now X_test became a numpy array)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on test set...\n",
      "...Done.\n",
      "[ 40276.39678298 121717.19636174  64338.45120398  65251.29005792\n",
      " 117077.25342622]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predictions on test set\n",
    "print(\"Predictions on test set...\")\n",
    "y_test_pred = regressor.predict(X_test)\n",
    "print(\"...Done.\")\n",
    "print(y_test_pred[:5])\n",
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Evaluate your model üå°Ô∏è"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can visualize our results and evaluate our model. Visualizing in graph is optional because depending on your model, it will be harder to build a 2D graph. However, we can always evaluate performance using a metric. \n",
    "\n",
    "For regression, we often use $R^2$. Let's do that using `sklearn` again. To evaluate our model, we will try to compare $R^2$ on the train set and on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Assessing the performances of the model ---\n",
      "R2 score on training set :  0.9470411990902998\n",
      "R2 score on test set :  0.9892203639925101\n"
     ]
    }
   ],
   "source": [
    "# Performance assessment\n",
    "print(\"--- Assessing the performances of the model ---\")\n",
    "\n",
    "# Print R^2 scores\n",
    "print(\"R2 score on training set : \", regressor.score(X_train, y_train))\n",
    "print(\"R2 score on test set : \", regressor.score(X_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the results, we will be able to tell if the model is performing well and whether it is overfitting or not. \n",
    "\n",
    "* $R^2$ close to 1 means good performance \n",
    "* $R^2_{train}$ > $R^2_{test}$ means overfitting \n",
    "* $R^2_{train}$ < $R^2_{test}$ means underfitting \n",
    "\n",
    "If you want to read more on overfitting in our blog: üëâ [What is overfitting?](https://www.jedha.co/blog/quest-ce-que-loverfitting) üëà"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Feature importance ü•ï\n",
    "\n",
    "Finally, what is interesting would be to see which features are important in your model. To do so, you can check out your model's coefficients! \n",
    "\n",
    "üö® If you want to interpret coefficients, make sure you NORMALIZED your data in the preprocessing part üö®"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's see how we can see how we can check out our coefficients: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficients are:  [ -290.80396519 -2182.15818671  2472.9621519  23904.2028923 ]\n",
      "Constant is:  73498.28163844894\n"
     ]
    }
   ],
   "source": [
    "print(\"coefficients are: \", regressor.coef_) \n",
    "print(\"Constant is: \", regressor.intercept_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the coefficients, we need to know which columns are associated with each one. If you look at `X_train` (or `X_test`), here is what you have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  1.        ,  1.75832984],\n",
       "       [ 0.        ,  0.        ,  1.        , -0.40973925],\n",
       "       [ 0.        ,  0.        ,  1.        ,  0.09356251],\n",
       "       [ 1.        ,  0.        ,  0.        ,  1.10016601],\n",
       "       [ 0.        ,  0.        ,  1.        , -0.83560996]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5] # Visualize the first 5 rows of your X_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at your data, you can deduce that the columns are in the following order:\n",
    "\n",
    "1. France\n",
    "2. Germany \n",
    "3. United Kingdom \n",
    "4. Years of Experience (Normalized)\n",
    "\n",
    "But how can we show it in a DataFrame? Well first, we need to use the [`.categories_`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html?highlight=one%20hot%20encoder#sklearn.preprocessing.OneHotEncoder) attribute from [`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html?highlight=one%20hot%20encoder#sklearn.preprocessing.OneHotEncoder). \n",
    "\n",
    "Since we use `ColumnTransformer`, we need to access `OneHotEncoder` using `.transformers_` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All transformers are:  [('cat', OneHotEncoder(), [0]), ('num', StandardScaler(), [1])]\n",
      "One Hot Encoder transformer is:  OneHotEncoder()\n"
     ]
    }
   ],
   "source": [
    "# Access transformers from feature_encoder\n",
    "print(\"All transformers are: \", feature_encoder.transformers_)\n",
    "\n",
    "# Access one specific transformer\n",
    "print(\"One Hot Encoder transformer is: \", feature_encoder.transformers_[0][1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can simply check the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns are:  [array(['France', 'Germany', 'United-Kingdom'], dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "# Print categories\n",
    "categorical_column_names = feature_encoder.transformers_[0][1].categories_\n",
    "print(\"Categorical columns are: \", categorical_column_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can concatenate them with the numerical column names. We will use `numeric_features` variable to determine the name of our columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical columns are:  Index(['YearsExperience'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "numerical_column_names = X.iloc[:, numeric_features].columns # using the .columns attribute gives us the name of the column \n",
    "print(\"numerical columns are: \", numerical_column_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to concatenate our `numerical_column_names` and our `categorical_column_names`. The easiest way to do it is by using [np.append](https://numpy.org/doc/stable/reference/generated/numpy.append.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['France', 'Germany', 'United-Kingdom', 'YearsExperience'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Append all columns \n",
    "all_column_names = np.append(categorical_column_names, numerical_column_names)\n",
    "all_column_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally rank all columns by importance using coefficients ü•∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_names</th>\n",
       "      <th>coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>-290.803965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Germany</td>\n",
       "      <td>-2182.158187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>United-Kingdom</td>\n",
       "      <td>2472.962152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YearsExperience</td>\n",
       "      <td>23904.202892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature_names  coefficients\n",
       "0           France   -290.803965\n",
       "1          Germany  -2182.158187\n",
       "2   United-Kingdom   2472.962152\n",
       "3  YearsExperience  23904.202892"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature importance \n",
    "feature_importance = pd.DataFrame({\n",
    "    \"feature_names\": all_column_names,\n",
    "    \"coefficients\":regressor.coef_\n",
    "})\n",
    "\n",
    "feature_importance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you know that you model is: \n",
    "\n",
    "$y=23904\\times YearsExperience + 2472 \\times UK - 2182 \\times Germany  -290\\times France + 73498$\n",
    "\n",
    "You can be very fancy and try to use `seaborn` to visualize feature importance üåü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x17e81425af0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set coefficient to absolute values to rank features\n",
    "feature_importance[\"coefficients\"] = feature_importance[\"coefficients\"].abs()\n",
    "\n",
    "# Visualize ranked features using seaborn\n",
    "sns.catplot(x=\"feature_names\", \n",
    "            y=\"coefficients\", \n",
    "            data=feature_importance.sort_values(by=\"coefficients\", ascending=False), \n",
    "            kind=\"bar\",\n",
    "            aspect=16/9) # Resize graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources üìöüìö\n",
    "\n",
    "* Linear and Quadratic Discriminant Analysis - [https://bit.ly/20Csax](https://scikit-learn.org/stable/modules/lda_qda.html)\n",
    "\n",
    "* What is overfitting - [https://bit.ly/30Cssas](https://www.jedha.co/blog/quest-ce-que-loverfitting)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "216d08ced86f1f6e0b5764233bcb18334be12ba95b6ee555f60be9cf0be8c147"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
